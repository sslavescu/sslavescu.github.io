Experience in EY, August 2020 to present day
For Volvo, we were tasked with developing their AI strategy, which involved three main streams:
1.	The Value stream: Determining how AI could enhance Volvo’s products and services, and improve their business processes.
2.	Governance and People and Skills stream: Establishing the necessary governance changes to integrate AI effectively. This also included understanding the impact on current jobs at Volvo, determining future job roles, and identifying areas where employees might need re-skilling.
3.	Architecture steam: This was my area of involvement, focused on defining the technical capabilities required across Volvo's entire enterprise architecture to support AI-driven solutions. Initially, Volvo had a simplistic view, seeing AI as just another box within their architecture diagram. However, AI needs to integrate more comprehensively into products, function as standalone services or applications, and either be developed in-house or integrated as plug-ins into existing business processes.
My role was to build a cohesive and enterprise-wide set of technical capabilities that would allow Volvo to develop and deploy AI-driven solutions effectively.
The vision I developed and agreed upon with the steering group and senior management translated into four key principles for the architecture design:
1.	Adaptability with Current and Future AI Developments: We aimed to define the technical capabilities based on the current understanding of AI, incorporating advancements from both academia and industry. Crucially, the capabilities needed to be flexible enough to adapt to future developments in the AI space.
2.	Federated Deployment: The envisioned system's capabilities would follow a federated approach. This meant that while the capabilities would be defined, configured, and initialized centrally, individual teams, departments, or business units could deploy and use these instances in their specific applications. Despite this distributed usage, control and management would remain centralised, which is not as decentralised as the industry definition of federated deployment but more to a hub-n-spoke approach.
3.	Robust Foundation: Adherence to the principles of the existing enterprise architecture, referred to as “North Star”, was essential. The architecture needed to be comprehensive, covering all stages of development and deployment without any assumptions. This robust foundation would ensure the capabilities were not just temporary solutions but integral parts of the enterprise architecture that are well-defined and sustainable.
4.	Responsible Design: Given the particular challenges introduced by AI, it was vital to emphasize transparency and explainability. These features ensure that products are understandable to their operators. Additionally, principles such as sustainability, fairness, and robustness, which were already part of the enterprise architecture design, were to be maintained. However, transparency and explainability were explicitly highlighted as essential elements within this vision to ensure responsible AI design.
By adhering to these principles, we aimed to establish a coherent and sustainable approach to integrating AI into Volvo's enterprise architecture, ensuring flexibility, robustness, and responsibility in design.
Over the next three to four months, I focused on assembling the necessary technical capabilities for AI integration into Volvo's systems and solutions. To simplify understanding and tracking developments, we categorized our approach into two streams: Generative AI and Analytical AI (this was the Volvo’s name for Predictive AI).
For Generative AI, we considered various capabilities such as document parsing and analysis, synthetic data generation, embeddings management (including tokenization and OCR), vector database indexing, prompt engineering, fine-tuning, hosting and cataloguing LLMs, enhancing transparency and explainability, human feedback loop capability, and more. These were envisioned not only as solutions but as enterprise-wide capabilities, meaning they could be built, provisioned, and used across the entire organisation.
Similarly, for Analytical AI, we outlined essential enterprise-wide capabilities in the traditional domain of AI, which encompasses machine learning. This included building new models, feature creation and storage in a feature store, dataset creation from mixed data sources, feature preparation, and supporting exploratory data analysis. We also developed mechanisms for cataloguing algorithms and tracking previous experiments to capture lessons learned.
Some capabilities overlapped between Generative and Analytical AI, such as synthetic data generation, feature stores, model catalogues, etc. I’ve also emphasised necessary capabilities to provide transparency and explainability for Analytical AI solutions.
In summary, I compiled a comprehensive list of AI capabilities, providing detailed explanations on their purpose and how they should be implemented and utilised as enterprise-wide resources. This way, different solutions can instantiate and leverage these AI capabilities effectively.
We’ve also made sure to account for other services and capabilities already defined within the enterprise architecture, such as application integration, API management, data classification, data management, DevSecOps, and more. These components, though not AI-specific, are crucial for successfully implementing AI solutions. I outlined these foundation services within the architecture diagram I developed for Volvo.
The next phase of this engagement involved creating an investment plan and an implementation roadmap. Initially, we estimated the costs for implementing the AI-specific capabilities using t-shirt sizing approach. We’ve also estimated the effort necessary to adapt the foundation services to requirements for supporting the AI-specific capabilities, and for this we’ve engaged with different teams within Volvo that were responsible for implementing or had already implemented the foundational services mentioned above.
For instance, we consulted with the team responsible for API management to ensure these foundational services were adequately prepared to support AI applications. Some services were already fully capable (marked as green), while others required minor adjustments.
After these consultations, we developed an investment plan detailing the costs associated with deploying these AI capabilities. We then created an implementation roadmap and presented it to the senior executive leadership.
As a result of our compelling and comprehensive case for the new AI strategy, along with the detailed and clear investment plan, the Senior Executive Leadership team agreed to invest in its implementation. Consequently, I have been reengaged by Volvo to help define the parameters, including the necessary guardrails and guidelines, for implementing and using these technical capabilities. Additionally, I have been asked to further elaborate on the capabilities and develop a more detailed investment plan and roadmap for fully implementing the enterprise architecture aspects of the AI strategy over the next two and a half to three years.
Bid CoPilot
I have also worked on a proof of concept (PoC) to enhance the process of creating responses to RFPs by leveraging the experience and knowledge gained from previous submissions. The project was aimed specifically at assisting the government tenders and engagements group within EY's technology consultancy.
Utilising generative AI, I developed a solution that analysed previously submitted RFPs. This AI-enabled system allowed team members to input their questions and receive tailored responses based on historical data, thereby accelerating the bid creation process.
Initially, I collaborated with a developer to design and code the solution. However, I eventually took over the project entirely when the developer had to shift to another task. The project reached a stage where our leadership recognised its potential as a valuable tool. The government tenders team was particularly impressed with the results and is eager to adopt it. The solution is slated for internal implementation and production soon.
Additional to my role as software architect I’ve also led the small development team and also collaborated closely with the Government engagements team to identify areas for improvement in the solution. I also managed expectations from the managing partners and crafted tailored communications to secure their support. This was crucial in ensuring continued development and adoption of the solution as an internal accelerator. Furthermore, we positioned it as a sales process accelerator to demonstrate our expertise in generative AI technology.
AIB RPA
As a Software Development Manager, I led a small, globally distributed team of 4 or 5 members to deliver an RPA (Robotic Process Automation) solution for AIB Bank, focusing primarily on customers' applications for new banking products and overall customer relationship management. My responsibilities included managing the team, coordinating with the bank to ensure our solution met their needs, and overseeing the implementation process.
Our team operated from various locations: India, Singapore, Belfast, Dublin, and Limerick. We successfully streamlined AIB’s business processes through our RPA developments. One of the main challenges I faced was navigating the complexities of the bank's operations, particularly integrating our solution with the existing IT infrastructure and the customer experience team.
Although AIB already had an RPA framework in place, it was confined to their IT department and had not been extended to the customer experience team. We collaborated with the IT department to deploy the necessary technology, enhancing the customer experience team’s capabilities. The most significant challenge was deploying the RPA solutions in production, as this involved dealing with various external concerns and aligning them with our business process requirements.
A considerable part of my effort was spent managing different stakeholders within the bank and understanding the implications of their requirements on our deliverables. One notable issue was obtaining test data for our RPA implementation. We needed real or pre-existing customer applications and forms to ensure the accuracy of our robots' output. However, this data was not readily available in a format that facilitated comprehensive testing.
For instance, testing new card applications was difficult because once an application was processed, we couldn't re-use it for testing. Moreover, creating a comprehensive testing environment with all the necessary business systems involved in the process was a complex task. Despite these challenges, I worked closely with other departments to implement the required systems and establish a proper testing environment.
Ultimately, this project taught me extensively about the intricacies of the bank’s internal operations and how to effectively deliver a robust RPA solution.
HSE
I briefly collaborated with HSE to explore the automation of their report generation process for submission to external regulatory bodies, such as the Ministry of Health. The focus was on reports related to expenditure and fund allocation across different projects. My primary goal was to understand their existing processes and then streamline them to enable automatic report generation. To achieve this, I devised a solution and developed a PoC that incorporated their existing applications, particularly leveraging SharePoint and other Microsoft tools.

NEOM
I’ve worked on crafting responses to requests for tender and proposals, including a significant and complex solution for a navigational port. The Saudi Arabian government aimed to expand and digitize an old port to create a fully digital and future-proof facility. The request for proposal (RFP) required an integration architecture to support this digital transformation.
To address this, I designed an event-driven architecture that integrated Internet of Things (IoT) devices from boats, cranes, and other port equipment with various systems such as Customer Relationship Management (CRM), finance, invoicing, and human resources. The solution utilized technologies like Kafka, Kafka Connect, and an IoT gateway to facilitate efficient data processing and communication.
In addition to developing the architecture, I formulated an investment plan and an implementation roadmap to support the proposed solution. This included addressing budgetary constraints and mitigating uncertainties associated with the new systems to be acquired as part of the wider digital transformation as well as integrating unknown interfaces for some legacy systems.

Revenue
Another proposal I’ve worked on was for an RFP from the revenue office, aimed at migrating their existing document management system from an on-premises solution to SharePoint in the cloud. One of their major challenges was the presence of numerous documents whose contents and necessity were unclear. This required a thorough, manual, review to determine whether each document should be migrated or archived on-premises.
To address this, I developed a solution leveraging the latest advancements in Generative AI to summarize documents, understand their content, and assess their relevance. Additionally, I designed a Machine Learning system that could learn from human decisions to automatically determine whether a document should be migrated or archived based on the summary extracted by the Generative AI component. I also created a proof of concept (PoC) to demonstrate the solution to the revenue office, which was well-received
Experience in Truata, June 2020 to August 2022
At Truata, I served as the Engineering Manager for the newly formed Web & Integration team. My responsibilities were twofold:
1.	Establishing a robust engineering framework for all Web and Integration related software developments at Truata. This involved implementing an appropriate Software Development Life Cycle (SDLC) to ensure the delivery of high-quality, innovative software solutions, identify the right tools for supporting this SDLC and, obviously, to hire a team and build a culture of high performance in software development.
2.	Providing software architecture expertise. I employed the right design patterns and components needed to integrate data engineering solutions that applied privacy-enhancing algorithms to large datasets with an innovative, responsive, and excellent front-end, which the team was also responsible for developing.
When I joined, the team comprised of 2 permanent employees and 2 external contractors, from one of our partners. Truata was focused on developing and offering two primary products in the market:
1.	A privacy-enhanced analytics platform originally built for MasterCard but adaptable for various clients.
2.	Calibrate, the flagship product. Calibrate is a suite of tools, algorithms, and facilities designed to process big data, identify potential privacy risks, recommend possible transformation functions to mitigate these risks and to apply them to the dataset in order to produce a privacy-safe data set that retained as much of the initial value in the data as possible. Calibrate's functionality extended beyond identifying Personally Identifiable Information (PII) in data sets, it also aimed to detect whether a person's digital fingerprint could be identified or created from various data columns across multiple data sets owned by a company. This ensured that companies could assess their data for privacy concerns before using it to build machine learning models or business intelligence systems.
Software Engineering Management experience in Truata
I will describe my engineering management approach for building the necessary culture, work environment and SDLC to develop a high performing team.
The initial or incumbent SDLC had been based the team dynamic using Scrum, we had a Scrum Master who was in charge of making sure the team understands Scrum and applies it as best as possible. My role here was to identify the best way I could help the team to develop the design that was approved for the software products to be developed but also to ensure the teams’ ability to accommodate change as, understandable for a start-up, the products’ requirements were changing quite often as the company was trying to clearly identify the right product features to build at the right time. I had worked with the Scrum Master to make sure I had the right level of access to be in the position to help and develop, as people manager, the engineers in my team but also to allow them the freedom to produce the best code they can and to enjoy the challenges while they were doing it. 
I have also worked with the DevOps team to put in place the right components to implement a continuous delivery pipeline for each product we were building. This included the branching strategy in Git, the code quality assurance tools and configurations, the right testing approach for unit testing and the corresponding code coverage, for integration testing and the UAT.
I had also worked with the Security team to ensure that the development process ensures the right level of secure coding as well as the appropriate engagement of security testing (PEN testing) at the right time and to cover the necessary security risks (OWASP top 10).
An important part of establishing a successful SDLC and a coherent culture within Truata was to align with the engineering manager of the Data Engineering team and with the Chief Data Scientist who managed the Data Science team. We had appropriate communication channels and managed the teams in similar ways, it actually worked quite well and when the Data Engineering manager left I’ve continued as Manager for that team too.
The resulting SDLC was based on BDD, which worked very well with the Product Manager as he was able to describe the functionality using the Given-When-Then language and our engineers understood how to implement the necessary functionality as well as the necessary tests for that functionality. I had also instilled a culture of developing quality software through developing comprehensive integration and user-acceptance tests. I had proposed that we do not have a separate testing team, but engineers in all teams would be either developing functionality code or testing code during a sprint, obviously not for the same product feature. In this way we achieved 2 very important goals in developing high quality software. Firstly, we had code for most of our tests, manual tests were quite a few and only in emergency situations and only for short time. Secondly, the engineers had to understand other features of the application, from a user’s perspective as they had to implement those tests.
The culture I have encouraged was one of open knowledge sharing, open conversations, constructive criticism and through understanding of architectural decisions. We also had as many non-work related meetings as possible as a team, we were working through the hight of the COVID pandemic, and I had maintained a culture of open door, virtual door most of the time, through the fact that I had encouraged my colleagues and team mates to call me on Teams whenever they need without having to block my calendar or ask for permission in any way, I was trying to implement “walking to one’s desk” virtually.
I had also encouraged engineers to freely find solutions for their problems and even to change architectural decisions in order to support their approach of producing a better product, obviously they will be supporting their approach with appropriate technical arguments and the necessary debates will be allowed and sometimes encouraged, as long as we maintain the focus of delivering the best product possible. If the proposed change had proven  benefits, I would even support it in discussions with the Product Manager to amend the delivery roadmap.
From June 2020 until August 2022 I have built and developed a high-performing Web & Integrations team that was able to deliver the two products described above and managed to add new features and work with product managers and clients to provide the expected features at a very high standard of performance, user experience and costs. I have relied on 3 very experienced engineers, which I have hired into the team and they provided expertise in UI, Java backend and runtime environments respectively. I have grown the Web & Integration team from 4 people to a 12 strong engineers team and then managed to retain it to 10 engineers even if the sales process for the 2 products was not as substantial as expected. 
Software Architecture experience in Truata
I will describe the approach I have used for designing the architecture for the 2 products mentioned above.
My guiding principles, patterns and practices were the ones described in the Domain Driven Desing approach and the Reactive Manifesto for implementing Reactive Systems.
I had tried to identify the bounding contexts, context maps, create the necessary ubiquitous language and the appropriate domain events and entities. Then used the Reactive Manifesto’s principles: Responsive, Elastic, Message Driven and Resilient to employ the right patterns, modules and components to implement these principles.
For the Privacy-Enhanced Analytics Platform (PEAP) I’ve developed an event-driven architecture using Angular as the front-end, the senior engineer I’ve hired for the role of driving the UI team was an expert in Angular, micros-services built in Java and deployed using containers and RabbitMQ as the message broker for the solution. The data engineering side was implemented using this Data Science platform called Knime which was a front-end to run and managed Spark jobs. The Data Engineering solution was a bit more complex that it needed to be, but at the time was the “legacy” way of running Jupyter notebooks in a Spark cluster environment. The main reason why I’ve proposed using an event-driven integration was to make sure that this data engineering platform is abstracted away, through events, so that it can be replaced with a more simpler solution in the future.
For the Calibrate product I’ve designed a similar architecture mainly because of the long-running data processing elements of the solution and because we needed to provide a way of accommodating any eventual implementation of data processing cluster that the client might already have in place, they might have a Databricks subscription or their own Spark Cluster environment or some other environment like Apache Beam or Apache Flink
The Calibrate product was offering tools for identifying privacy risks within any type of  dataset, including digital fingerprinting of a person, and transformation solutions to mitigate the identified risks.
I’ve created the architecture for integrating the existing data processing algorithms so that they can be applied to large datasets, by multiple users with the expected user experience related to long-running jobs, such as the ability of monitoring a job as it’s progressing, to alert of any issues in the dataset as soon as they appear and to allow users to cancel any long-running jobs without losing out on the findings identified.
The application was structured using the micro-kernel pattern, having a central piece of functionality which allowed the integration of diverse plugins for different algorithms and data processing solutions as well as client based Identity Management components, Security Key Vaults, data warehouse, data lake or any other data storing facility. The product structure allowed for a flexible licensing model and flexible upgrading.
I have also worked with the DevOps engineers in the team to deploy the application as a SaaS product within Azure Marketplace and IBM’s Cloud Pack 4 Data.
For both these applications I’ve included an internal Certification Authority which was used to provide Mutual TLS identification, using digital certificates, for all communication channels between majority of components of both applications, especially when connecting to RabbitMQ.
I’ve also proposed and designed and abstraction layers for data access facility in Calibrate that will not only allow for any data source to be accommodated for both ingestion and output but also to allow for data streaming input and output.
Authorisation mechanism was relying on signed JWTs and authorisation claims to allow the integration of any client specific IdM as well as for building a stand-alone product using an internal Keycloak IdM.
Work experience in Jaguar LandRover (JLR) Jan 2019 to June 2020
Software Engineering Management
I was brought in as software development manager to grow and develop 6 engineers engaged in a few separate projects, of which I was running only one, the Digital Payments Gateway product called Digital Wallet.
As people manager I had worked with the engineers under my direction to align their career aspirations with the company’s development plans. Through regular 1-1s and ad-hoc meetings I helped them align their career aspirations with the company’s business goals.
I have also helped them strike the right balance between working on developing the newly created development centre in Shannon with the development of relationships with the other team members and, to forge new friendships.
Some of the engineers in my team wanted to pivot their careers from embedded applications development to web applications development and I had facilitated that transition. Some of them had just graduated from University and didn’t have a lot of experience and I have guided them to try different types of possible careers and choose the one that suited the most. I had also worked with very experienced engineers and had to make sure their expertise is properly used in the products and development processes we engaged in. I had also increased and encouraged people’s visibility within the wider software engineering group in the company, linking them with colleagues in other development centres in the UK and US
Software Architecture experience in JLR
This initial project I started with was the establishment of the internal digital payments gateway, Digital Wallet, which was an idea that started in the development centre in Shannon. This idea was initially thought as part of a suite of features that the Entertainment unit in the JLR cars will provide, the ability to make digital payments while on the road, to pay for tolls, fuel, fines, etc. While working on this we identified the more pressing need that the JLR enterprise had for providing digital payments for internal and customer facing applications. Therefore, we evolved the design of the Digital Wallet to be a Digital Payments Gateway, Because we needed to accommodate payments initiated in the JLR cars, for which the Internet connectivity may be patchy, I’ve opted for a design following the Reactive Systems principles and patterns. The design was reliant on integration components that provided the necessary features to submit and complete payments from any application whether deployed on the JLR enterprise or in the JLR cars. It had also provided an abstraction layer that could integrate any digital payments provider, we started with Stripe first. We made sure that the integration was done in such a way that the digital wallet would not require any credit card detail to be stored in the JLR infrastructure, using the facilities provided by the payments providers and reducing the burden of PCI DSS compliance.
The architecture used RabbitMQ as the message broker, with Pub/Sub pattern integrated via binding and routing keys. The events generated by the components were stored in JSON format with some of the sensitive elements encrypted, when necessary. All components were connecting to RabbitMQ via TLS and we’ve created an internal Certification Authority to provide the authorisation mechanism of subscribing to topics via Mutual TLS. 
All components of the application were packaged as Docker containers and we’ve deployed some of them in an Kubernetes cluster for elastic scalability.
I had also worked with the engineers in the team to put together a continuous integration pipeline using Gitlab hooks, and we evolved that into a Continuous Deployment pipeline using containers and GKE.
The Digital wallet was eventually deployed on the JLR GCP private cloud and some PoCs were using it. It was an important component of the emerging Digital Platform for Connected Car (Domain 9).
After the Digital Wallet entered a “business as usual” mode I had pivoted to support the Enterprise Architecture team to design the data and integration architecture for the newly started new Digital Platform for Connected Car (Domain 9). Given my previous experience in Integration platforms I had designed the architecture to support cars to stream data into the platform and for integration of other enterprise applications such as the Digital Wallet, CRM, Financial applications, etc. The integration involved both synchronous and asynchronous paradigms. I have designed the ReST based API integration necessary components such as API gateway, management, integration with SSO and internal Identity Management system. Designed the necessary message broker based integration for Pub/Sub or Command/Response messaging approaches.
I have also worked on the design for a Digital Twin approach that would create and maintain a view of every JLR car produced with information about their status, mileage, and any other sensor data.
After the design was finalised I have proposed and established the Platform Reliability Engineering team. This team’s remit was to ensure all development teams that were using the digital platform will have access to its services in a reliable way, ensuring aspects like scalability, observability, secure communication channels, consistent runtime between all stages of development and testing, predictable performance. This team was ensuring robustness and reliability at runtime as well as providing support and expertise with necessary software architecture patterns to be employed by product development teams, so that the resulting applications will be reliably operating in production.
I had developed the roadmap for this team in collaboration with all the stakeholders, which varied from product managers of the software products to be developed on the platform, the development teams for the platform to the business teams owning the other applications in the JLR ecosystem that needed access to vehicle data and platform applications.
University of Limerick, Sept 2018 to Dec 2018
A short but very fruitful stint with University of Limerick where I was charged with designing a data architecture and the implementation roadmap to ensure the reliability of data across the University departments. Part of this process I have identified all the sources of data within the departments in the University residing in data repositories or other alternative facilities. After that collated the data requirements and data confidence levels in all the departments in the University. All this information was then used to create an enterprise data architecture design and implementation roadmap.
Allied Irish Banks, Jan 2017 to Sept 2018
Software Architecture experience
I have been part of the architecture team for the new Digital Business Banking application, which was a new implementation of an existing application for corporate banking. My role was to design the architecture of the platform to integrate the front-end application built using Backbase, with the core banking systems (accounts, payments, fraud detection, etc.) This platform included a template for all integration services, which had taken care of cross-cutting, quality concerns, such as observability, security, reliability, scalability. The services offered a ReST interface for the front-end and connected with the internal banking systems in multiple ways, via ReST, asynchronous messaging and direct SDK-based calls. I had also worked very closely with the software engineers to translate the architecture into quality code.
An important aspect of my role there was to introduce the principles of Continuous Integration with the goal of improving the release process and to achieve Continuous Delivery. Part of this challenge I had worked with the Scrum masters, the internal Quality Assurance team and the infrastructure team to set up the necessary artefacts, development process and architectural designs. The result was that we had managed to achieve Continuous Integration, including integration and user acceptance tests, but we failed to overcome the issues posed by the shared database and the internal infrastructure provisioning processes, to achieve the Continuous Delivery goal.
Software Development Management experience
This was the first time when I ventured into the role of People Manager along-side the technical lead role by taking on the position of Software Development Manager for the development team, release team and the migration team, 15-18 engineers.
My role as development manager was to align these 3 teams for a smoother operation of releasing new features and partially transferring features that clients used from the old application to the new one. Apart from the technical challenges that this problem posed I had to deal with the people challenges and to make sure the 3 teams operated as one.
I had regular 1-1s even though all but 1 engineers in the team were contractors with the purpose of maintaining and increasing the teams’ performance and collaboration
AXA Experience Sept 2014 to Jan 2017
I was brought in as Architect part of the Enterprise Architecture team with the scope of improving the integration between the applications in the ecosystem and for a better understand and use of data. Shortly after I have started the CIO lunched this challenge “The 5-day App” with the scope of improving the processes and the applications underlying the policy and claims management systems so that a new feature or product will be available to customers in 5 days, or less. The existing policies and claims platform was a suite of Oracle Forms applications and PL/SQL code, for which the configuration was maintained in a very complicated suite of Excel spreadsheets. Obviously, such an architecture wouldn’t be conducive to quick changes. My answer to the challenge was to design a modular solution which will address different part of the business process and which will communicate with each other asynchronously to allow for variable processing speed. At the time the ideas of microservices and Domain Driven Design were emerging, and I had come across them through the O’Reilly Learning Platform (at the time operating as Safari Books Online), for which I’m a subscriber from its inception. Utilising these new ideas of decomposing business process and implementing modular and distributed applications I had designed an architecture that implemented the “Strangler” pattern (I didn’t know it had a name at the time). We were utilising functionality already implemented in the old system, for policy management, but we have created a new set of services that were providing the necessary functionality to offer a new service to customers. We were collecting the data associated with this new product through the new application and we have implemented a few wrappers around the existing data structures within the existing policy management application, that were able to process events produced by the new application and to produce events when the new policies were added to the existing system or whenever any errors occurred.
The new architecture lent itself to implementing a CI/CD pipeline, which we have managed to put in place in quite a short time. In the end, after about 4 months of work, with a team of 3 people, including me, we have managed to deliver this new architecture, with the appropriate CI/CD pipeline, working in the Dev environment and, as proof of our architecture completing the 5-day app, we were able to add a new payment method based on Stripe, to the application in less than 5 days.
As a result of this achievement I was tasked with designing a new architecture for a new Digital Platform which included new Policies and Claims systems using the lessons learnt during the implementation of the 5-day App challenge. This architecture was an event-driven design, relying on microservices to produce and consume events to do with CRUD operations on policies or claims. We were also able to use the new Digital Platform design to better integrate our business with our clients, other financial institutions resellers of our insurance products, as we could accommodate multiple consumers to same event, for product updates or other such changes, and we could also accommodate specific integration requirement for each client, if necessary.
When the acquisition by AXA occurred, I was tasked with providing architectural design to the solution that will ensure the convergence of the 2 companies on the existing platform within Genworth. I was trying at the same time to develop the new Digital Platform and integrate the 2 companies via this new solution but the business pressures were too strong to allow for this, hence we had to converge using the old Policies and Claims platform, which was a very frustrating process.
As part of my role as an architect I was working very closely with the development team, not only in the 5-day App but to maintain the existing Policies and Claims applications. I had provided technical mentorship to developers in that team as they were navigating the challenges posed by the paradigm shift from working on a monolithic application to an events-based distributed and modular architecture.
Work experience in Oracle, June 2007 to Sept 2014
I had started as Senior Engineer maintaining and improving an existing application that translators around the world were using to get packages of work from Oracle, to add translations of English content into their language of expertise and then return the texts so that they can be loaded into the internal Translation Factory which was the main product to streamline the product translation process. This effort required a lot of C++ development, using an old version of Microsoft Visual Studio 6, MFC libraries and other such tools especially an internal Oracle database. There were two major improvements that I’ve added to the product. One was to migrate the internal character set from Windows specific one to UTF8, which involved a lot of changes in memory allocation approaches, classes for manipulating text, MFC libraries version, etc. This was recognised a very important milestone in the life of the product as it was the main enabler to accommodate new languages which were not covered by the default Windows character set.
The second major improvement was migrating the internal Oracle database, including the OCI driver used, from version 7 to version 8. On the face of it the change was quite small but the OCI 8 driver had a significant amount of changes versus the OCI 7 driver including the type of variables used from int 16 rather than int 8 to char 32 rather than char 8 and other such changes of fundamental impact.
At the same time with working on the Translators tool I had an interest in Artificial Intelligence for Translation and looked at a few options out there including an internal project that used a Rules Based Machine Learning solution, which didn’t produce important improvements. In my research I came across this Statistical Machine Learning open-source product called Moses. After evaluating its potential I had put together a plan and got management approval to start a small team to look into using the internal database of translations from Translation Factory to train an Oracle specific machine translation solution using this product. We had initially started with a small PoC that used a few “easy” languages like Italian and French. We took English texts and produced suggestions, which were assessed by the internal Oracle language specialists for those languages. These translations had a high level of accuracy that helped us make a case for building this solution in production for 5 languages, amongst which we’ve tackled the English to Japanese language translations too. Over the next 4 years the team that I formed got expanded with more language specialists and developers and we achieved productivity improvements between 10-20% for the 5 languages. My work was to understand the technical challenges that the open source solution was struggling with in translating for the 5 languages chosen, and design solutions to make the translations’ quality better. These solutions involved data preparation tools to operate on what was quite large text corpus, some changes to the statistical algorithms used for mapping n-grams between different languages, and solutions in productionising the Machine Translation within the existing process, especially for adding machine generated suggestions to the work packages for translators as well as monitoring the quality of our suggestions over time, comparing them with the result produced by humans and improving our confidence level into the quality of our suggestions.
When Oracle management in US decided to build an internal NLP group we started sharing the work we’ve done with them and they had used some of our libraires and data preparation products. Not long after, the decision by the management of the Worldwide Product Translation Group, which I was part of, was to put the Machine Translation solution in Business as Usual mode and wait for the NLP group in US to produce a new solution. At that point I have pivoted to working on integrating the Machine Translation system with the Translation Factory via ReST based APIs, so I got involved in designing and implementing these services to automate the Translation Process.
Work Experience Original Solutions, Feb 2004 to June 2007
In Original Solution my role was as Senior Applications Consultant.
My most important engagement was to define the architecture and lead the software development efforts for the Web Application front-end to the internally-built, Oracle Forms and PL/SQL based solution called Unified Gateway, which empowered other telecom operators in the country to benefit from selling telecommunication services  as a result of the opening up of the telecommunication network, decentralisation of these services, managed by the previously state-owned Eir(Eircom at the time). Part of the process I had also defined processes, methodologies and tools for the design, development and deployment phases of the project.
Another project that I’ve worked on while in Original Solutions was as J2EE technical consultant for Mainframe Integration. I had designed an enterprise service using EJBs to provide access to a legacy system built on an IBM mainframe using NEON Systems Shadow technology.
Apart from these projects I had also been involved in supporting as software architect various other consultancy engagements with other customers.
Work experience in Orygen, April 2001 to May 2003
Orygen was the Irish subsidiary of a large telecommunications US company called Level 3. When the Dot Com boom finished the US company decided to withdraw from Ireland, and some of the people in Orygen formed Original Solutions.
My role in Orygen was as Senior Software Architect.
The last engagement I have worked on in Orygen, before it closed down, was a Customer Relationship Management Portal application for an important Health Insurance British company operating in Ireland (Bupa). I was the main software architected and I had lead the implementation team for  improvements to an existing, in-house built, CRM system.
Before that project I had been the Software Architect and Lead Developer for Corporate Banking Online application for Bank of Ireland. I was responsible with designing and implementing new functionality or improvements for existing Java application and the middleware integration layer and mentoring a small team of Java developers, ex mainframe developers, who wanted to continue their careers in Internet technologies.
Work experience between May 2003 until Feb 2004.
Once Orygen closed down I have managed to work on a short, 3 months contract with Lionbridge and then joined Clare County Council as Analyst programmer. Here I had designed and developed an Extranet to Sharepoint integration tool using C# and ASP.NET. This application was used for automatic files publishing from specified locations on the Intranet, based on Sharepoint, to a protected location on the Extranet Web server. 
Another solution I have built in Clare County Council was a Resource Booking application using C#, ASP.NET and MS SQL server used for managing the allocation of the large county council meeting room for improved occupancy and usage.
For the period of 1996 until 2001 I had worked in a few software development and Internet service providers in Romania. In the later part of this period, between 1998 and 2001 I had been working in 3 companies at the same time.
My main position was that of Senior Software Engineer in Multinet where I had architected and implemented web and desktop applications for a multitude of domains (tourism, communications, banking) using OOP, XP, Java, J2EE, Pascal, C++
I had also been Technical Director in a new start-up that I had been a founder of, along-side other people, which provided Internet services in the local town. I had put together the technical infrastructure to resale the Internet service to businesses and individuals in our town.
I had also set up a company with my wife to build web applications, called Lynet, which operated between May 2000 until April 2001. In this company I had been the Java and J2EE Software architect and development team Lead for a multitude of web applications using J2EE 1.2, MySQL server, JavaScript, Apache Struts. I had also mentored a small team of developers in emerging technologies like Java and J2EE.
Spare time projects
In my spare time, I am continuing to enhance my knowledge in software engineering both the architecting side as well as team management.
In the software architecture I have always been interested in modular and distributed systems and best way to implement them to address the challenges of performance, data consistency, scalability and user experience. I have been a long time subscriber of the O’Reilly learning platform and in Autumn 2023 I had decided to join a few other developers and set up a team to compete. We had managed to bring our solution to the semi-final, which was quite a success given that the team members haven’t know each other before this and had quite different expertise and levels of experience. The solution we’ve proposed can be viewed at https://github.com/WildSight-Wizards/wildlifewatcher . My contribution there was to build the software architecture of integrating the cameras distributed across the globe in a seamless way, I had borrowed from my experience in Jaguar LandRover where I’ve designed the Digital Twin solution.
I also like exploring the challenges posed by explaining how AI systems operate and how can they be safely integrated with humans, via innovative software architectures, to enhance the software solutions I develop. I have recently graduated a Masters in Artificial Intelligence, which gave me the necessary theoretical foundation to better understand how AI can be built, how ML works and how the inferences can be integrated. Continuing from there I want to understand how we can integrate feedback loops that allow application users to get answers to questions about how the application that employs an AI component has come up to the specific decisions it made. It is a complex field of study involving Explainable AI as well as understanding how deep learning algorithms model the patterns that form the foundation of the generated predictions.